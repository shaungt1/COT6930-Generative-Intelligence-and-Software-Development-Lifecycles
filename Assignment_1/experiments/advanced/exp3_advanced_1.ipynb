{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Hybrid Prompting for Business Evaluation\n",
    "\n",
    "This advanced hybrid prompting method enables an AI assistant to systematically evaluate software business ideas through **iterative user interaction** and **structured analysis**. The model follows a three-phase approach:\n",
    "\n",
    "1. **Phase 1 - Data Collection:** The AI gathers all necessary user-provided information by asking targeted questions sequentially. Each response is stored and recalled to avoid redundancy.\n",
    "2. **Phase 2 - Internal Processing:** The AI processes all collected data without external interaction, applying structured business evaluation techniques, including market analysis, feasibility assessment, and risk identification.\n",
    "3. **Phase 3 - Final Report Generation:** The AI synthesizes all findings into a comprehensive **business viability report**, detailing market relevance, technical feasibility, financial projections, and strategic recommendations.\n",
    "\n",
    "The hybrid approach allows for **dynamic refinement**, meaning the AI injects structured evaluation sections progressively, refining insights based on user responses before generating the final verdict. This ensures **higher accuracy**, **minimized information gaps**, and **actionable business insights** compared to traditional static prompting techniques.\n",
    "\n",
    "The prompt employs **Dynamic Reasoning with Augmented Action and Memory (DRAAM)**, integrating **iterative questioning, real-time memory recall, structured inference, and logical evaluation** to assess software business ideas. It follows a **three-phase methodology**, beginning with **context-aware user input collection**, ensuring only relevant questions are asked dynamically. The system then moves into **iterative analysis**, where structured sections use **pattern-based reasoning, market comparisons, and hypothesis-driven assessments** to determine feasibility, scalability, and market potential. The final phase compiles all insights into a **comprehensive evaluation report**, leveraging **computed TAM, SAM, and SOM** metrics, risk assessments, and business viability scoring. The methodology ensures **adaptive, real-world feasibility analysis** while maintaining a **concise, structured approach**, dynamically adjusting responses based on the user's input and previous interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Simulated AI Response: `<SO_System_Role_and_Persona>`\n",
      "### System Role\n",
      "YOU MUST TAKE ON THIS ROLE AS FOUNDR AI THE AI BOT WH...\n",
      "AI: Simulated AI Response: `<SO_System_Role_and_Persona>`\n",
      "### System Role\n",
      "YOU MUST TAKE ON THIS ROLE AS FOUNDR AI THE AI BOT WH...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ai_response)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Capture user input\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Append user response to history\u001b[39;00m\n\u001b[0;32m     50\u001b[0m conversation_history\u001b[38;5;241m.\u001b[39mappend(user_input)\n",
      "File \u001b[1;32mc:\\Users\\Frontside\\Documents\\2. FAU PHD COURSES\\COT6930 -GenAI\\COT6930-Generative-Intelligence-and-Software-Development-Lifecycles\\PROMPTLABENV\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Frontside\\Documents\\2. FAU PHD COURSES\\COT6930 -GenAI\\COT6930-Generative-Intelligence-and-Software-Development-Lifecycles\\PROMPTLABENV\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from modules.system_prompt import system_prompt\n",
    "from modules.phaseI import SYS_PHASEI\n",
    "\n",
    "# Function to create the LLM request payload\n",
    "def create_payload(target, model, prompt, temperature, num_ctx, num_predict):\n",
    "    return {\n",
    "        \"target\": target,\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"num_ctx\": num_ctx,\n",
    "        \"num_predict\": num_predict\n",
    "    }\n",
    "\n",
    "# Function to send a request to the model\n",
    "def model_req(payload):\n",
    "    # Simulated model request, should be replaced with actual API request\n",
    "    response = f\"Simulated AI Response: {payload['prompt'][:100]}...\"\n",
    "    time_taken = round(time.time() % 1.5, 3)  # Simulated response time\n",
    "    return time_taken, response\n",
    "\n",
    "# Combine system prompt with Phase 1 instructions\n",
    "PHASE1_PROMPT = system_prompt + \"\\n\" + SYS_PHASEI\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    # Construct the prompt dynamically\n",
    "    PROMPT =  PHASE1_PROMPT \n",
    "\n",
    "    # Generate payload for LLM\n",
    "    payload = create_payload(\n",
    "        target=\"ollama\",\n",
    "        model=\"llama2:latest\",\n",
    "        prompt=PROMPT,\n",
    "        temperature=1.0,\n",
    "        num_ctx=100,\n",
    "        num_predict=300\n",
    "    )\n",
    "\n",
    "    # Send request to LLM\n",
    "    time_taken, ai_response = model_req(payload)\n",
    "    print(\"AI:\", ai_response)\n",
    "\n",
    "    # Capture user input\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Append user response to history\n",
    "    conversation_history.append(user_input)\n",
    "\n",
    "    # Check if user signals completion\n",
    "    if user_input.strip().upper() == \"CALL FINISH_CHAT\":\n",
    "        print(\"\\n--- PHASE 1 COMPLETE ---\\n\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from _pipeline import create_payload, model_req\n",
    "from modules.system_prompt import system_prompt\n",
    "from modules.phaseI import SYS_PHASEI\n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "MESSAGE = system_prompt.format(template_function=SYS_PHASEI)\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "PROMPT = MESSAGE \n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:3b\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=1.0, \n",
    "                         num_ctx=100, \n",
    "                         num_predict=100)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Simulated response to: `<SO_System_Role_and_Persona>`\n",
      "### System Role\n",
      "YOU...\n",
      "AI: Simulated response to: `<SO_System_Role_and_Persona>`\n",
      "### System Role\n",
      "YOU...\n",
      "AI: Simulated response to: `<SO_System_Role_and_Persona>`\n",
      "### System Role\n",
      "YOU...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ai_response)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Capture user input\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Append user input to history\u001b[39;00m\n\u001b[0;32m     51\u001b[0m conversation_history\u001b[38;5;241m.\u001b[39mappend(user_input)\n",
      "File \u001b[1;32mc:\\Users\\Frontside\\Documents\\2. FAU PHD COURSES\\COT6930 -GenAI\\COT6930-Generative-Intelligence-and-Software-Development-Lifecycles\\PROMPTLABENV\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Frontside\\Documents\\2. FAU PHD COURSES\\COT6930 -GenAI\\COT6930-Generative-Intelligence-and-Software-Development-Lifecycles\\PROMPTLABENV\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "# Import necessary system configurations\n",
    "from modules.system_prompt import system_prompt\n",
    "\n",
    "# Function to create a payload for the LLM\n",
    "def create_payload(target, model, prompt, temperature, num_ctx, num_predict):\n",
    "    return {\n",
    "        \"target\": target,\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"num_ctx\": num_ctx,\n",
    "        \"num_predict\": num_predict\n",
    "    }\n",
    "\n",
    "# Function to send a request to the model\n",
    "def model_req(payload):\n",
    "    # Simulate a model request (This should be replaced with actual API request)\n",
    "    response = f\"Simulated response to: {payload['prompt'][:50]}...\"\n",
    "    time_taken = round(time.time() % 1.5, 3)  # Simulated response time\n",
    "    return time_taken, response\n",
    "\n",
    "# Initialize an empty list to store conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Begin Phase 1: User Input Collection\n",
    "while True:\n",
    "    # Construct the prompt dynamically using conversation history\n",
    "    PROMPT = system_prompt + \"\\nUser: \" + \"\\n\".join(conversation_history)\n",
    "\n",
    "    # Generate payload for LLM\n",
    "    payload = create_payload(\n",
    "        target=\"ollama\",\n",
    "        model=\"llama2:latest\",\n",
    "        prompt=PROMPT,\n",
    "        temperature=1.0,\n",
    "        num_ctx=100,\n",
    "        num_predict=300\n",
    "    )\n",
    "\n",
    "    # Send to LLM and capture response\n",
    "    time_taken, ai_response = model_req(payload)\n",
    "    print(\"AI:\", ai_response)\n",
    "\n",
    "    # Capture user input\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Append user input to history\n",
    "    conversation_history.append(user_input)\n",
    "\n",
    "    # If user signals completion, break and move to Phase 3\n",
    "    if user_input.strip().upper() == \"CALL FINISH_CHAT\":\n",
    "        print(\"\\n--- PHASE 1 COMPLETE ---\\n\")\n",
    "        break\n",
    "\n",
    "# Store Phase 1 and 2 responses for analysis\n",
    "phase_data = \"\\n\".join(conversation_history)\n",
    "\n",
    "# Begin Phase 3: Iterative Processing of Analysis Reports\n",
    "PHASE3_PROMPT = system_prompt + \"\\n[Phase 1 and 2 Data Collected]\\n\"\n",
    "\n",
    "for i in range(1, 11):  # Iterating through 10 structured reports\n",
    "    print(f\"\\nProcessing Report Section {i}...\\n\")\n",
    "\n",
    "    # Append specific Phase 3 output template to prompt dynamically\n",
    "    phase3_prompt = PHASE3_PROMPT + f\"\\n[Processing ANALYZE_REPORT{i}]\\n\"\n",
    "\n",
    "    payload = create_payload(\n",
    "        target=\"ollama\",\n",
    "        model=\"llama2:latest\",\n",
    "        prompt=phase3_prompt,\n",
    "        temperature=1.0,\n",
    "        num_ctx=100,\n",
    "        num_predict=500  # Increased token count for report generation\n",
    "    )\n",
    "\n",
    "    time_taken, report_output = model_req(payload)\n",
    "    print(\"AI Report:\", report_output)\n",
    "\n",
    "# Generate Final Business Evaluation Report\n",
    "FINAL_REPORT_PROMPT = system_prompt + \"\\n[Generating FINAL BUSINESS REPORT]\\n\"\n",
    "\n",
    "payload = create_payload(\n",
    "    target=\"ollama\",\n",
    "    model=\"llama2:latest\",\n",
    "    prompt=FINAL_REPORT_PROMPT,\n",
    "    temperature=1.0,\n",
    "    num_ctx=200,\n",
    "    num_predict=700\n",
    ")\n",
    "\n",
    "time_taken, final_report = model_req(payload)\n",
    "print(\"\\n--- FINAL BUSINESS REPORT ---\\n\", final_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROMPTLABENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
